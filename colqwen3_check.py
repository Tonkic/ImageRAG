import torch
from transformers import AutoModel, AutoProcessor, AutoConfig
from transformers.dynamic_module_utils import get_class_from_dynamic_module
from PIL import Image
import requests
import gc

# ================= é…ç½®éƒ¨åˆ† =================
LOCAL_MODEL_PATH = "TomoroAI/tomoro-colqwen3-embed-8b"
TARGET_DEVICE_ID = 3  # æ‚¨æŒ‡å®šçš„æ˜¾å¡ ID

# æž„é€ è®¾å¤‡å­—ç¬¦ä¸²
DEVICE_STR = f"cuda:{TARGET_DEVICE_ID}"

# ================= çŽ¯å¢ƒæ¸…ç†ä¸Žå‡†å¤‡ =================
# 1. æ¸…ç†æ˜¾å­˜ï¼Œé˜²æ­¢ä¹‹å‰çš„æŠ¥é”™æ®‹ç•™
gc.collect()
torch.cuda.empty_cache()

print(f"ðŸš€ å¯åŠ¨é«˜æ€§èƒ½æ¨¡å¼ (Native BF16)")
print(f"ðŸŽ¯ ä½¿ç”¨è®¾å¤‡: {DEVICE_STR} (RTX 3090)")

# ================= æ ¸å¿ƒä¿®å¤å‡½æ•° (ä¸‡èƒ½ Patch) =================
def load_model_high_performance(model_path, device_str):
    """
    ä»¥åŽŸç”Ÿ BF16 åŠ è½½æ¨¡åž‹ï¼Œå¹¶ä¿®å¤æ‰€æœ‰å‚æ•°å…¼å®¹æ€§é—®é¢˜ã€‚
    """
    print("ðŸ› ï¸  æ­£åœ¨ Patch æ¨¡åž‹ä»£ç ä»¥é€‚é…æ–°ç‰ˆ Transformers...")

    # 1. èŽ·å–æ¨¡åž‹ç±»å®šä¹‰
    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
    class_ref = config.auto_map["AutoModel"]
    model_class = get_class_from_dynamic_module(class_ref, model_path)

    # 2. åº”ç”¨ä¸‡èƒ½è¡¥ä¸ (åžæŽ‰æ‰€æœ‰ä¸è®¤è¯†çš„å‚æ•°)
    if hasattr(model_class, "tie_weights"):
        original_tie_weights = model_class.tie_weights

        def safe_tie_weights(self, **kwargs):
            # è¿™é‡Œçš„ **kwargs ä¼šæ•èŽ· missing_keys, recompute_mapping ç­‰æ‰€æœ‰å‚æ•°
            # æˆ‘ä»¬ä¸ä¼ ç»™åŽŸå‡½æ•°ï¼Œç›´æŽ¥ä¸¢å¼ƒï¼Œä»Žè€Œé¿å… TypeError
            return original_tie_weights(self)

        model_class.tie_weights = safe_tie_weights
        print("âœ… tie_weights Patch æˆåŠŸ (å·²å±è”½æ‰€æœ‰æœªçŸ¥å‚æ•°)")

    # 3. åŠ è½½æ¨¡åž‹ (BF16 + Flash Attention 2)
    print(f"ðŸ”¥ æ­£åœ¨åŠ è½½å®Œæ•´æ¨¡åž‹ (BF16)... è¿™éœ€è¦çº¦ 16GB æ˜¾å­˜")
    model = model_class.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,        # å…³é”®ï¼šä½¿ç”¨åŽŸç”Ÿ BF16
        attn_implementation="flash_attention_2", # å…³é”®ï¼šå¼€å¯åŠ é€Ÿ
        trust_remote_code=True,
        device_map={"": device_str}        # å¼ºåˆ¶æŒ‡å®šå•å¡
    ).eval()

    return model

# ================= ä¸»æµç¨‹ =================

# 1. åŠ è½½å¤„ç†å™¨
processor = AutoProcessor.from_pretrained(
    LOCAL_MODEL_PATH,
    trust_remote_code=True,
    max_num_visual_tokens=1280,
)

# 2. åŠ è½½æ¨¡åž‹
try:
    model = load_model_high_performance(LOCAL_MODEL_PATH, DEVICE_STR)
    print(f"âœ¨ æ¨¡åž‹åŠ è½½æˆåŠŸï¼æ˜¾å­˜å ç”¨æ­£å¸¸ã€‚")
except Exception as e:
    print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    exit()

# ================= å‡†å¤‡æ•°æ® =================
queries = [
    "Retrieve the city of Singapore",
    "Retrieve the city of Beijing",
]

def load_image(url):
    try:
        response = requests.get(url, stream=True, timeout=10)
        return Image.open(response.raw).convert("RGB")
    except Exception:
        print(f"æ— æ³•ä¸‹è½½å›¾ç‰‡: {url}")
        return Image.new('RGB', (224, 224), color='gray')

print("ðŸ“¥ æ­£åœ¨åŠ è½½å›¾ç‰‡...")
image_urls = [
    "https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Singapore_skyline_2022.jpg/640px-Singapore_skyline_2022.jpg",
    "https://upload.wikimedia.org/wikipedia/commons/thumb/6/61/Beijing_skyline_at_night.JPG/640px-Beijing_skyline_at_night.JPG"
]
images = [load_image(url) for url in image_urls]

# ================= æŽ¨ç† (é«˜æ€§èƒ½æ¨¡å¼) =================
print("ðŸ§  å¼€å§‹æŽ¨ç† (BF16 Precision)...")

# æ–‡æœ¬ç¼–ç 
batch_queries = processor.process_texts(queries)
batch_queries = {k: v.to(DEVICE_STR) for k, v in batch_queries.items()}

with torch.inference_mode():
    query_outputs = model(**batch_queries)
    # ä¿æŒåœ¨ GPU ä¸Šè¿›è¡Œæ‰“åˆ†è®¡ç®—ä¼šæ›´å¿«ï¼Œæœ€åŽå†è½¬ CPU
    query_embeddings = query_outputs.embeddings

# å›¾ç‰‡ç¼–ç 
batch_images = processor.process_images(images)
batch_images = {k: v.to(DEVICE_STR) for k, v in batch_images.items()}

with torch.inference_mode():
    image_outputs = model(**batch_images)
    doc_embeddings = image_outputs.embeddings

# ================= æ‰“åˆ† =================
# æ³¨æ„ï¼šscore_multi_vector å¯èƒ½ä¼šåœ¨ CPU ä¸Šè¿è¡Œï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ tensor åœ¨ CPU
# æˆ–è€…å¦‚æžœåº“æ”¯æŒ GPU è®¡ç®—ï¼Œå¯ä»¥å°è¯•ä¸è½¬ã€‚ä¸ºäº†ç¨³å¦¥ï¼Œè¿™é‡Œè½¬å›ž CPUã€‚
scores = processor.score_multi_vector(
    query_embeddings.to(torch.float32).cpu(),
    doc_embeddings.to(torch.float32).cpu()
)

print("\n=== æ£€ç´¢ç»“æžœ ===")
for i, query in enumerate(queries):
    print(f"\nðŸ” æŸ¥è¯¢: '{query}'")
    for j, url in enumerate(image_urls):
        print(f"   -> å›¾ç‰‡ {j+1} åˆ†æ•°: {scores[i][j].item():.4f}")