import os
import sys

import argparse
import torch
from diffusers import ZImagePipeline

def parse_args():
    parser = argparse.ArgumentParser(description="Z-Image-Turbo Multi-GPU Inference Demo")

    parser.add_argument("--device", type=int, default=0, help="æŒ‡å®šä½¿ç”¨çš„ GPU ID (ä¾‹å¦‚: 0, 1, 2, 3)")
    parser.add_argument("--prompt", type=str, default="Young Chinese woman in red Hanfu, intricate embroidery, soft lighting, 8k resolution", help="æç¤ºè¯")
    parser.add_argument("--output", type=str, default="result.png", help="è¾“å‡ºæ–‡ä»¶å")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")

    return parser.parse_args()

def main():
    args = parse_args()

    # æ£€æŸ¥ CUDA
    if not torch.cuda.is_available():
        raise RuntimeError("æœªæ£€æµ‹åˆ° CUDA è®¾å¤‡ã€‚")

    # æ£€æŸ¥ GPU ID æ˜¯å¦è¶Šç•Œ
    num_gpus = torch.cuda.device_count()
    if args.device >= num_gpus:
        raise ValueError(f"é”™è¯¯ï¼šè®¾å¤‡ ID {args.device} è¶…å‡ºèŒƒå›´ (å¯ç”¨: {num_gpus})")

    device_str = f"cuda:{args.device}"
    gpu_name = torch.cuda.get_device_name(args.device)
    print(f"ğŸš€ æ˜¾å¡: [{device_str}] {gpu_name}")

    # åŠ è½½æ¨¡å‹
    print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹ (ä½¿ç”¨æœ¬åœ°è·¯å¾„ ./Z-Image-Turbo)...")
    model_path = "./Z-Image-Turbo"

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}ï¼Œè¯·ç¡®è®¤æ–‡ä»¶å¤¹åç§°æ˜¯å¦æ­£ç¡®ã€‚")

    pipe = ZImagePipeline.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=False,
        local_files_only=True
    )

    pipe.to(device_str)

    # å°è¯•å¼€å¯ Flash Attention
    try:
        pipe.transformer.set_attention_backend("flash")
        print("âœ… Flash Attention å·²å¯ç”¨")
    except Exception:
        print("âš ï¸ æœªå¯ç”¨ Flash Attention (éè‡´å‘½é”™è¯¯)")

    print(f"âš¡ï¸ å¼€å§‹ç”Ÿæˆ: '{args.prompt}'")

    # å¿…é¡»ä¸º Generator æŒ‡å®šè®¾å¤‡
    generator = torch.Generator(device_str).manual_seed(args.seed)

    image = pipe(
        prompt=args.prompt,
        height=512,
        width=512,
        num_inference_steps=9,  # Turbo ä¸“å±æ­¥æ•°
        guidance_scale=0.0,     # Turbo ä¸“å± CFG
        generator=generator,
    ).images[0]

    image.save(args.output)
    print(f"ğŸ’¾ å®Œæˆ! å›¾ç‰‡å·²ä¿å­˜è‡³: {os.path.abspath(args.output)}")

if __name__ == "__main__":
    main()